{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: /TFM/DATASET-1/v2/csv/data.csv\n",
      "Total files found: 1\n",
      "Loaded file: data.csv, shape: (8656767, 86)\n",
      "Updated file saved: /TFM/DATASET-1/v2/updated_csv/data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Define the mappings for renaming columns\n",
    "column_mapping = {\n",
    "    \"Source IP\": \"Src IP\",\n",
    "    \"Source Port\": \"Src Port\",\n",
    "    \"Destination IP\": \"Dst IP\",\n",
    "    \"Destination Port\": \"Dst Port\",\n",
    "    \"Total Fwd Packets\": \"Total Fwd Packet\",\n",
    "    \"Total Backward Packets\": \"Total Bwd packets\",\n",
    "    \"Total Length of Fwd Packets\": \"Total Length of Fwd Packet\",\n",
    "    \"Total Length of Bwd Packets\": \"Total Length of Bwd Packet\",\n",
    "    \"Min Packet Length\": \"Packet Length Min\",\n",
    "    \"Max Packet Length\": \"Packet Length Max\",\n",
    "    \"Avg Fwd Segment Size\": \"Fwd Segment Size Avg\",\n",
    "    \"Avg Bwd Segment Size\": \"Bwd Segment Size Avg\",\n",
    "    \"CWE Flag Count\": \"CWR Flag Count\",\n",
    "    \"Init_Win_bytes_forward\": \"FWD Init Win Bytes\",\n",
    "    \"Init_Win_bytes_backward\": \"Bwd Init Win Bytes\",\n",
    "    \"act_data_pkt_fwd\": \"Fwd Act Data Pkts\",\n",
    "    \"min_seg_size_forward\": \"Fwd Seg Size Min\"\n",
    "}\n",
    "\n",
    "# Get the list of CSV file paths\n",
    "dspaths = []\n",
    "for dirname, _, filenames in os.walk('/TFM/DATASET-1/v2/csv/'):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.csv'):\n",
    "            pds = os.path.join(dirname, filename)\n",
    "            dspaths.append(pds)\n",
    "            print(f\"Found file: {pds}\")\n",
    "\n",
    "print(f\"Total files found: {len(dspaths)}\")\n",
    "\n",
    "# Load the CSV files into a dictionary of DataFrames\n",
    "data_frames = {}\n",
    "for file in dspaths:\n",
    "    file_name = os.path.basename(file)\n",
    "    data_frames[file_name] = pd.read_csv(file)\n",
    "    print(f\"Loaded file: {file_name}, shape: {data_frames[file_name].shape}\")\n",
    "\n",
    "    # Rename the columns according to the mapping\n",
    "    data_frames[file_name].rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "    # Update the 'Label' column based on the 'Traffic Subtype' and 'Label' values\n",
    "    data_frames[file_name]['Label'] = data_frames[file_name].apply(\n",
    "        lambda row: row['Traffic Subtype'] if row['Label'] == 'Malicious' else 'BENIGN' if row['Label'] == 'Benign' else row['Label'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Save the updated DataFrame back to CSV\n",
    "    output_path = os.path.join('/TFM/DATASET-1/v2/updated_csv/', file_name)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    data_frames[file_name].to_csv(output_path, index=False)\n",
    "    print(f\"Updated file saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo encontrado: /TFM/DATASET-1/v2/updated_csv/data.csv\n",
      "Total de archivos encontrados: 1\n",
      "Procesando archivo: data.csv\n",
      "Archivo actualizado guardado: /TFM/DATASET-1/v2/final_csv/data.csv, shape: (8656767, 77)\n",
      "Procesamiento completado.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de columnas a eliminar\n",
    "columns_to_remove = [\n",
    "    \"Flow ID\", \"Src IP\", \"Src Port\", \"Dst IP\", \"Dst Port\", \n",
    "    \"Timestamp\", \"Traffic Type\", \"Traffic Subtype\", \"CWR Flag Count\"\n",
    "]\n",
    "\n",
    "# Obtener la lista de rutas de los archivos CSV\n",
    "dspaths = []\n",
    "for dirname, _, filenames in os.walk('/TFM/DATASET-1/v2/updated_csv/'):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.csv'):\n",
    "            pds = os.path.join(dirname, filename)\n",
    "            dspaths.append(pds)\n",
    "            print(f\"Archivo encontrado: {pds}\")\n",
    "\n",
    "print(f\"Total de archivos encontrados: {len(dspaths)}\")\n",
    "\n",
    "# Cargar los archivos CSV y eliminar las columnas especificadas\n",
    "for file in dspaths:\n",
    "    file_name = os.path.basename(file)\n",
    "    print(f\"Procesando archivo: {file_name}\")\n",
    "    \n",
    "    # Leer el CSV en chunks para manejar archivos grandes\n",
    "    chunk_size = 100000  # Ajusta este valor según la memoria disponible\n",
    "    chunks = pd.read_csv(file, chunksize=chunk_size)\n",
    "    \n",
    "    # Procesar cada chunk\n",
    "    processed_chunks = []\n",
    "    for chunk in chunks:\n",
    "        # Verificar si las columnas a eliminar existen en el DataFrame\n",
    "        existing_columns_to_remove = [col for col in columns_to_remove if col in chunk.columns]\n",
    "        \n",
    "        # Eliminar las columnas no deseadas\n",
    "        chunk = chunk.drop(columns=existing_columns_to_remove)\n",
    "        \n",
    "        # Convertir todas las columnas a tipo string para evitar problemas de tipo de datos\n",
    "        chunk = chunk.astype(str)\n",
    "        \n",
    "        # Reemplazar posibles valores no válidos\n",
    "        chunk = chunk.replace({'nan': '', 'inf': '', '-inf': ''})\n",
    "        \n",
    "        processed_chunks.append(chunk)\n",
    "    \n",
    "    # Concatenar todos los chunks procesados\n",
    "    df = pd.concat(processed_chunks, ignore_index=True)\n",
    "    \n",
    "    # Guardar el DataFrame actualizado\n",
    "    output_path = os.path.join('/TFM/DATASET-1/v2/final_csv/', file_name)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8', quoting=1)\n",
    "    print(f\"Archivo actualizado guardado: {output_path}, shape: {df.shape}\")\n",
    "\n",
    "print(\"Procesamiento completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codigo momentaneo para calcular la longitud maxima del dataframe y los datos por etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: data.csv\n",
      "Unknown file type: data.csv\n",
      "Label: BENIGN, Number of samples: 0\n",
      "Label: Bruteforce, Number of malignant samples: 0\n",
      "Label: DoS, Number of malignant samples: 0\n",
      "Label: Information Gathering, Number of malignant samples: 0\n",
      "Label: Mirai, Number of malignant samples: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Crear un DataFrame con todas las muestras benignas\n",
    "\n",
    "# Initialize dictionaries for benign and malignant samples\n",
    "benign_samples = pd.DataFrame()\n",
    "malign_samples_by_label = {\n",
    "    'Bruteforce': pd.DataFrame(),\n",
    "    'DoS': pd.DataFrame(),\n",
    "    'Information Gathering': pd.DataFrame(),\n",
    "    'Mirai': pd.DataFrame()\n",
    "}\n",
    "\n",
    "# Process each DataFrame\n",
    "for file, df in data_frames.items():\n",
    "    print(f\"Processing file: {file}\")\n",
    "    if file in ['processed_Audio.csv', 'processed_Text.csv', 'processed_Video.csv', 'processed_Background.csv']:\n",
    "        # These files are considered BENIGN\n",
    "        benign_samples = pd.concat([benign_samples, df])\n",
    "        print(f\"Added {len(df)} rows to BENIGN\")\n",
    "    else:\n",
    "        # Determine the attack type based on the file name\n",
    "        if 'Bruteforce' in file:\n",
    "            attack_type = 'Bruteforce'\n",
    "        elif 'DoS' in file:\n",
    "            attack_type = 'DoS'\n",
    "        elif 'Information_Gathering' in file:\n",
    "            attack_type = 'Information Gathering'\n",
    "        elif 'Mirai' in file:\n",
    "            attack_type = 'Mirai'\n",
    "        else:\n",
    "            print(f\"Unknown file type: {file}\")\n",
    "            continue\n",
    "        \n",
    "        # Add samples to the corresponding attack type\n",
    "        malign_samples_by_label[attack_type] = pd.concat([malign_samples_by_label[attack_type], df])\n",
    "        print(f\"Added {len(df)} rows to {attack_type}\")\n",
    "        \n",
    "# Display the number of benign samples\n",
    "print(f\"Label: BENIGN, Number of samples: {len(benign_samples)}\")\n",
    "\n",
    "# Display the number of malignant samples per label\n",
    "for label, samples in malign_samples_by_label.items():\n",
    "    malign_count = len(samples)\n",
    "    print(f\"Label: {label}, Number of malignant samples: {malign_count}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realizamos la division de los datos en funcion de la distribuccion Gaussiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No malignant samples found. Skipping malignant sample processing.\n",
      "No benign samples found.\n",
      "\n",
      "Final sample counts:\n",
      "BENIGN: 0\n",
      "\n",
      "Total samples in all_data: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Gaussian distribution parameters\n",
    "mu = 0.5\n",
    "sigma = 0.2\n",
    "\n",
    "# Initialize dictionary for reduced malignant samples by label\n",
    "malign_reduced_samples_by_label = {}\n",
    "\n",
    "# Check if malign_samples_by_label is empty or has no non-zero categories\n",
    "if not malign_samples_by_label or all(len(samples) == 0 for samples in malign_samples_by_label.values()):\n",
    "    print(\"No malignant samples found. Skipping malignant sample processing.\")\n",
    "    min_samples = 0\n",
    "else:\n",
    "    # Find the minimum number of samples among non-zero categories\n",
    "    min_samples = min(len(samples) for label, samples in malign_samples_by_label.items() if len(samples) > 0)\n",
    "    print(f\"Target number of samples per attack type: {min_samples}\")\n",
    "\n",
    "    # Process each DataFrame\n",
    "    for label, samples in malign_samples_by_label.items():\n",
    "        if len(samples) == 0:\n",
    "            print(f\"Skipping {label} due to zero samples\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate the probability density function of each sample\n",
    "        samples['prob'] = norm.pdf(np.linspace(0, 1, len(samples)), mu, sigma)\n",
    "        \n",
    "        # Reduce samples to min_samples using the probability weights\n",
    "        if len(samples) > min_samples:\n",
    "            malign_reduced_samples_by_label[label] = samples.sample(n=min_samples, weights='prob', random_state=42)\n",
    "        else:\n",
    "            # If we have fewer samples than min_samples, use all available samples\n",
    "            malign_reduced_samples_by_label[label] = samples\n",
    "        \n",
    "        print(f\"Reduced {label} to {len(malign_reduced_samples_by_label[label])} samples\")\n",
    "\n",
    "    # Concatenate all reduced malignant samples into one DataFrame\n",
    "    malign_reduced_data = pd.concat(malign_reduced_samples_by_label.values())\n",
    "\n",
    "    # Drop the 'prob' column\n",
    "    malign_reduced_data = malign_reduced_data.drop('prob', axis=1)\n",
    "\n",
    "# Process benign samples\n",
    "if 'benign_samples' in locals() and not benign_samples.empty:\n",
    "    benign_data = benign_samples\n",
    "\n",
    "    # Reduce benign samples to match the number of samples per attack type\n",
    "    if min_samples > 0 and len(benign_data) > min_samples:\n",
    "        benign_data = benign_data.sample(n=min_samples, random_state=42)\n",
    "    print(f\"Reduced BENIGN to {len(benign_data)} samples\")\n",
    "\n",
    "    # Concatenate all benign and reduced malignant samples into one DataFrame\n",
    "    all_data = pd.concat([benign_data, malign_reduced_data]) if 'malign_reduced_data' in locals() else benign_data\n",
    "else:\n",
    "    print(\"No benign samples found.\")\n",
    "    all_data = malign_reduced_data if 'malign_reduced_data' in locals() else pd.DataFrame()\n",
    "\n",
    "print(\"\\nFinal sample counts:\")\n",
    "if 'benign_data' in locals():\n",
    "    print(f\"BENIGN: {len(benign_data)}\")\n",
    "for label, samples in malign_reduced_samples_by_label.items():\n",
    "    print(f\"{label}: {len(samples)}\")\n",
    "\n",
    "print(f\"\\nTotal samples in all_data: {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba (70% entrenamiento, 30% prueba)\n",
    "train_data, test_val_data = train_test_split(all_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Dividir el conjunto de prueba en conjuntos de prueba y validación (50% prueba, 50% validación)\n",
    "#test_data, val_data = train_test_split(test_val_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Guardar los conjuntos de entrenamiento en archivos CSV\n",
    "train_data.to_csv('train.csv', index=False)\n",
    "\n",
    "# Guardar los conjuntos de prueba en archivos CSV\n",
    "test_val_data.to_csv('test.csv', index=False)\n",
    "\n",
    "# Guardar los conjuntos de validación en archivos CSV\n",
    "#val_data.to_csv('validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Crear un DataFrame con todas las muestras benignas\\n#benign_samples = pd.concat([df for df in data_frames.values() if df.iloc[:, 77].eq(\\'BENIGN\\').any()])\\nbenign_samples = pd.concat([df[df.iloc[:, 77].eq(\\'BENIGN\\')] for df in data_frames.values()])\\n##benign_samples = benign_samples.sample(n=10000, random_state=42)\\nprint(f\"Length: {len(benign_samples)}\")\\n# Reducir las muestras malignas de cada fichero utilizando la función gaussiana para seleccionar las muestras\\nmu = 0.5  # Media de la distribución gaussiana\\nsigma = 0.2  # Desviación estándar de la distribución gaussiana\\n\\nmalign_samples_reduced = {}\\nfor file, df in data_frames.items():\\n    # Obtener solo las muestras malignas (sin etiqueta BENIGN)\\n    malign_samples = df[df.iloc[:, 77] != \\'BENIGN\\']\\n\\n    # Calcular las probabilidades gaussianas\\n    #malign_samples[\\'prob\\'] = norm.pdf(np.linspace(0, 1, len(malign_samples)), mu, sigma)\\n    ##malign_samples.loc[:, \\'prob\\'] = norm.pdf(np.linspace(0, 1, len(malign_samples)), mu, sigma)\\n\\n    # Reducir las muestras malignas a 10000 por fichero\\n    ##malign_samples_reduced[file] = malign_samples.sample(n=10000, weights=\\'prob\\', random_state=42)\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Crear un DataFrame con todas las muestras benignas\n",
    "#benign_samples = pd.concat([df for df in data_frames.values() if df.iloc[:, 77].eq('BENIGN').any()])\n",
    "benign_samples = pd.concat([df[df.iloc[:, 77].eq('BENIGN')] for df in data_frames.values()])\n",
    "##benign_samples = benign_samples.sample(n=10000, random_state=42)\n",
    "print(f\"Length: {len(benign_samples)}\")\n",
    "# Reducir las muestras malignas de cada fichero utilizando la función gaussiana para seleccionar las muestras\n",
    "mu = 0.5  # Media de la distribución gaussiana\n",
    "sigma = 0.2  # Desviación estándar de la distribución gaussiana\n",
    "\n",
    "malign_samples_reduced = {}\n",
    "for file, df in data_frames.items():\n",
    "    # Obtener solo las muestras malignas (sin etiqueta BENIGN)\n",
    "    malign_samples = df[df.iloc[:, 77] != 'BENIGN']\n",
    "\n",
    "    # Calcular las probabilidades gaussianas\n",
    "    #malign_samples['prob'] = norm.pdf(np.linspace(0, 1, len(malign_samples)), mu, sigma)\n",
    "    ##malign_samples.loc[:, 'prob'] = norm.pdf(np.linspace(0, 1, len(malign_samples)), mu, sigma)\n",
    "\n",
    "    # Reducir las muestras malignas a 10000 por fichero\n",
    "    ##malign_samples_reduced[file] = malign_samples.sample(n=10000, weights='prob', random_state=42)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los ficheros con datos malignos todos juntos y benigno. Lo cramos una sola vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Concatenar todos los DataFrames con datos benignos en un solo DataFrame\\nbenign_data = pd.concat([benign_samples])\\n\\n# Concatenar todos los DataFrames con datos malignos reducidos en un solo DataFrame\\nmalign_reduced_data = pd.concat(malign_samples_reduced)\\n\\n# Eliminar la columna \"prob\" del DataFrame malign_reduced_data\\nmalign_reduced_data = malign_reduced_data.drop(\\'prob\\', axis=1)\\n\\n# Guardar los datos benignos en un archivo CSV\\nbenign_data.to_csv(\\'benign_data.csv\\', index=False)\\n\\n# Guardar los datos malignos reducidos en un archivo CSV sin la columna \"prob\"\\nmalign_reduced_data.to_csv(\\'malign_reduced_data.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Concatenar todos los DataFrames con datos benignos en un solo DataFrame\n",
    "benign_data = pd.concat([benign_samples])\n",
    "\n",
    "# Concatenar todos los DataFrames con datos malignos reducidos en un solo DataFrame\n",
    "malign_reduced_data = pd.concat(malign_samples_reduced)\n",
    "\n",
    "# Eliminar la columna \"prob\" del DataFrame malign_reduced_data\n",
    "malign_reduced_data = malign_reduced_data.drop('prob', axis=1)\n",
    "\n",
    "# Guardar los datos benignos en un archivo CSV\n",
    "benign_data.to_csv('benign_data.csv', index=False)\n",
    "\n",
    "# Guardar los datos malignos reducidos en un archivo CSV sin la columna \"prob\"\n",
    "malign_reduced_data.to_csv('malign_reduced_data.csv', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "División entre Train, Test y Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.model_selection import train_test_split\\n\\n# Concatenar los datos benignos y malignos en un solo DataFrame\\nall_data = pd.concat([benign_data, malign_reduced_data])\\n\\n# Dividir los datos en conjuntos de entrenamiento y prueba (70% entrenamiento, 30% prueba)\\nall_train, all_test = train_test_split(all_data, test_size=0.3, random_state=42)\\n\\n# Dividir el conjunto de prueba en conjuntos de prueba y validación (20% prueba, 10% validación)\\nall_test, all_val = train_test_split(all_test, test_size=0.5, random_state=42)\\n\\n# Separar los datos benignos y malignos en los conjuntos correspondientes\\nbenign_train = all_train[all_train.iloc[:, 77] == 'BENIGN']\\nmalign_train = all_train[all_train.iloc[:, 77] != 'BENIGN']\\nbenign_test = all_test[all_test.iloc[:, 77] == 'BENIGN']\\nmalign_test = all_test[all_test.iloc[:, 77] != 'BENIGN']\\nbenign_val = all_val[all_val.iloc[:, 77] == 'BENIGN']\\nmalign_val = all_val[all_val.iloc[:, 77] != 'BENIGN']\\n\\n# Guardar los conjuntos de entrenamiento en archivos CSV\\npd.concat([benign_train, malign_train]).to_csv('train.csv', index=False)\\n\\n# Guardar los conjuntos de prueba en archivos CSV\\npd.concat([benign_test, malign_test]).to_csv('test.csv', index=False)\\n\\n# Guardar los conjuntos de validación en archivos CSV\\npd.concat([benign_val, malign_val]).to_csv('validation.csv', index=False)\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Concatenar los datos benignos y malignos en un solo DataFrame\n",
    "all_data = pd.concat([benign_data, malign_reduced_data])\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba (70% entrenamiento, 30% prueba)\n",
    "all_train, all_test = train_test_split(all_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Dividir el conjunto de prueba en conjuntos de prueba y validación (20% prueba, 10% validación)\n",
    "all_test, all_val = train_test_split(all_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Separar los datos benignos y malignos en los conjuntos correspondientes\n",
    "benign_train = all_train[all_train.iloc[:, 77] == 'BENIGN']\n",
    "malign_train = all_train[all_train.iloc[:, 77] != 'BENIGN']\n",
    "benign_test = all_test[all_test.iloc[:, 77] == 'BENIGN']\n",
    "malign_test = all_test[all_test.iloc[:, 77] != 'BENIGN']\n",
    "benign_val = all_val[all_val.iloc[:, 77] == 'BENIGN']\n",
    "malign_val = all_val[all_val.iloc[:, 77] != 'BENIGN']\n",
    "\n",
    "# Guardar los conjuntos de entrenamiento en archivos CSV\n",
    "pd.concat([benign_train, malign_train]).to_csv('train.csv', index=False)\n",
    "\n",
    "# Guardar los conjuntos de prueba en archivos CSV\n",
    "pd.concat([benign_test, malign_test]).to_csv('test.csv', index=False)\n",
    "\n",
    "# Guardar los conjuntos de validación en archivos CSV\n",
    "pd.concat([benign_val, malign_val]).to_csv('validation.csv', index=False)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
