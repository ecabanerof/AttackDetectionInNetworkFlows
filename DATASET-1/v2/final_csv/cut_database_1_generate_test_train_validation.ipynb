{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/TFM/DATASET-1/v2/final_csv\\data.csv\n",
      "/TFM/DATASET-1/v2/final_csv\\test_1.csv\n",
      "/TFM/DATASET-1/v2/final_csv\\train_1.csv\n",
      "File: data.csv, Shape: (8656767, 77)\n",
      "File: test_1.csv, Shape: (2597031, 77)\n",
      "File: train_1.csv, Shape: (6059736, 77)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Obtener la lista de rutas de los archivos CSV\n",
    "dspaths = []\n",
    "for dirname, _, filenames in os.walk('/TFM/DATASET-1/v2/final_csv'):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.csv'):\n",
    "            pds = os.path.join(dirname, filename)\n",
    "            dspaths.append(pds)\n",
    "            print(pds)\n",
    "\n",
    "# Cargar los archivos CSV en un diccionario de DataFrames\n",
    "data_frames = {}\n",
    "for file in dspaths:\n",
    "    file_name = os.path.basename(file)\n",
    "    df = pd.read_csv(file)\n",
    "    print(f\"File: {file_name}, Shape: {df.shape}\")\n",
    "    data_frames[file_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ecaba\\AppData\\Local\\Temp\\ipykernel_30080\\2720396241.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns=column_mapping, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: BENIGN, Number of samples: 2602\n",
      "Label: Bruteforce FTP, Number of malignant samples: 6970\n",
      "Label: Bruteforce SSH, Number of malignant samples: 7934\n",
      "Label: Bruteforce Telnet, Number of malignant samples: 9826\n",
      "Label: Bruteforce HTTP, Number of malignant samples: 1256\n",
      "Label: Bruteforce DNS, Number of malignant samples: 44358\n",
      "Label: DoS ICMP, Number of malignant samples: 18\n",
      "Label: DoS UDP, Number of malignant samples: 515988\n",
      "Label: DoS SYN, Number of malignant samples: 1713528\n",
      "Label: DoS FIN, Number of malignant samples: 1451200\n",
      "Label: DoS ACK, Number of malignant samples: 1872614\n",
      "Label: DoS PSH, Number of malignant samples: 1819014\n",
      "Label: DoS RST, Number of malignant samples: 2145008\n",
      "Label: DoS URG, Number of malignant samples: 1812380\n",
      "Label: DoS ECN, Number of malignant samples: 1742300\n",
      "Label: DoS CWR, Number of malignant samples: 1745046\n",
      "Label: DoS HTTP, Number of malignant samples: 164702\n",
      "Label: DoS MAC, Number of malignant samples: 60\n",
      "Label: Information Gathering, Number of malignant samples: 2076726\n",
      "Label: Mirai Scan Bruteforce, Number of malignant samples: 17462\n",
      "Label: Mirai DDoS UDP, Number of malignant samples: 142\n",
      "Label: Mirai DDoS ACK, Number of malignant samples: 7558\n",
      "Label: Mirai DDoS SYN, Number of malignant samples: 28420\n",
      "Label: Mirai DDoS GREIP, Number of malignant samples: 98\n",
      "Label: Mirai DDoS GREETH, Number of malignant samples: 86\n",
      "Label: Mirai DDoS HTTP, Number of malignant samples: 17846\n",
      "Label: Mirai DDoS DNS, Number of malignant samples: 110392\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to normalize column names\n",
    "def normalize_column_names(df):\n",
    "    column_mapping = {\n",
    "        'Fwd Bytes/Bulk Avg': 'Fwd Avg Bytes/Bulk',\n",
    "        'Fwd Packet/Bulk Avg': 'Fwd Avg Packets/Bulk',\n",
    "        'Fwd Packets/Bulk Avg': 'Fwd Avg Packets/Bulk', # Duplicate column name not really present\n",
    "        'Fwd Bulk Rate Avg': 'Fwd Avg Bulk Rate',\n",
    "        'Bwd Bytes/Bulk Avg': 'Bwd Avg Bytes/Bulk',\n",
    "        'Bwd Packet/Bulk Avg': 'Bwd Avg Packets/Bulk',\n",
    "        'Bwd Packets/Bulk Avg': 'Bwd Avg Packets/Bulk', # Duplicate column name not really present\n",
    "        'Bwd Bulk Rate Avg': 'Bwd Avg Bulk Rate'\n",
    "        \n",
    "    }\n",
    "    df.rename(columns=column_mapping, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Function to get the index of the label column\n",
    "def get_label_column_index(df):\n",
    "    # Check if 'Label' column exists\n",
    "    if 'Label' in df.columns:\n",
    "        return df.columns.get_loc('Label')\n",
    "    # If not, assume it's the last column\n",
    "    return df.shape[1] - 1\n",
    "\n",
    "# Crear un DataFrame con todas las muestras benignas\n",
    "benign_samples = pd.concat([normalize_column_names(df[df.iloc[:, get_label_column_index(df)] == 'BENIGN']) for df in data_frames.values()])\n",
    "print(f\"Label: BENIGN, Number of samples: {len(benign_samples)}\")\n",
    "\n",
    "# Gaussian distribution parameters\n",
    "mu = 0.5\n",
    "sigma = 0.2\n",
    "\n",
    "# Initialize dictionary for malignant samples by label\n",
    "malign_samples_by_label = {}\n",
    "\n",
    "# Process each DataFrame\n",
    "for file, df in data_frames.items():\n",
    "    # Normalize column names\n",
    "    df = normalize_column_names(df)\n",
    "    \n",
    "    label_col_index = get_label_column_index(df)\n",
    "    \n",
    "    # Standardize the label column by replacing 'UDP-lag' and 'UDP-Lag' with 'UDPLag'\n",
    "    df.iloc[:, label_col_index] = df.iloc[:, label_col_index].replace(['UDP-lag', 'UDP-Lag'], 'UDPLag')\n",
    "    \n",
    "    # Get only malignant samples and drop of WEBDDOS\n",
    "    malign_samples = df[(df.iloc[:, label_col_index] != 'BENIGN') & (df.iloc[:, label_col_index] != 'WebDDoS')]\n",
    "    \n",
    "    # Group by label and collect samples\n",
    "    for label in malign_samples.iloc[:, label_col_index].unique():\n",
    "        if label not in malign_samples_by_label:\n",
    "            malign_samples_by_label[label] = pd.DataFrame()\n",
    "        \n",
    "        malign_samples_by_label[label] = pd.concat([malign_samples_by_label[label], malign_samples[malign_samples.iloc[:, label_col_index] == label]])\n",
    "\n",
    "# Display the number of malignant samples per label\n",
    "for label, samples in malign_samples_by_label.items():\n",
    "    malign_count = len(samples)\n",
    "    print(f\"Label: {label}, Number of malignant samples: {malign_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducción de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom scipy.stats import norm\\n\\n# Gaussian distribution parameters\\nmu = 0.5\\nsigma = 0.2\\n\\n# Initialize dictionary for reduced malignant samples by label\\nmalign_reduced_samples_by_label = {}\\n\\n# Process each DataFrame\\nfor label, samples in malign_samples_by_label.items():\\n    # Calculate the probability density function of each sample\\n    samples[\\'prob\\'] = norm.pdf(np.linspace(0, 1, len(samples)), mu, sigma)\\n    \\n    # Reduce samples to 113828 using the probability weights\\n    if len(samples) > 113828:\\n        malign_reduced_samples_by_label[label] = samples.sample(n=113828, weights=\\'prob\\', random_state=42)\\n    #else:\\n        malign_reduced_samples_by_label[label] = samples\\n    \\n    print(f\"Label: {label}, Original samples: {len(samples)}, Reduced samples: {len(malign_reduced_samples_by_label[label])}\")\\n\\n# Concatenate all reduced malignant samples into one DataFrame\\nmalign_reduced_data = pd.concat(malign_reduced_samples_by_label.values())\\n\\n# Drop the \\'prob\\' column\\nmalign_reduced_data = malign_reduced_data.drop(\\'prob\\', axis=1)\\n\\n# Concatenate all benign samples into one DataFrame\\nbenign_data = benign_samples\\n\\n# Concatenate all benign and reduced malignant samples into one DataFrame\\nall_data = pd.concat([benign_data, malign_reduced_data])\\n\\nprint(f\"Total samples in final dataset: {len(all_data)}\")\\nprint(f\"Number of benign samples: {len(benign_data)}\")\\nprint(f\"Number of malignant samples: {len(malign_reduced_data)}\")\\nprint(\"\\nSample distribution:\")\\nprint(all_data.iloc[:, get_label_column_index(all_data)].value_counts())\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Gaussian distribution parameters\n",
    "mu = 0.5\n",
    "sigma = 0.2\n",
    "\n",
    "# Initialize dictionary for reduced malignant samples by label\n",
    "malign_reduced_samples_by_label = {}\n",
    "\n",
    "# Process each DataFrame\n",
    "for label, samples in malign_samples_by_label.items():\n",
    "    # Calculate the probability density function of each sample\n",
    "    samples['prob'] = norm.pdf(np.linspace(0, 1, len(samples)), mu, sigma)\n",
    "    \n",
    "    # Reduce samples to 113828 using the probability weights\n",
    "    if len(samples) > 113828:\n",
    "        malign_reduced_samples_by_label[label] = samples.sample(n=113828, weights='prob', random_state=42)\n",
    "    #else:\n",
    "        malign_reduced_samples_by_label[label] = samples\n",
    "    \n",
    "    print(f\"Label: {label}, Original samples: {len(samples)}, Reduced samples: {len(malign_reduced_samples_by_label[label])}\")\n",
    "\n",
    "# Concatenate all reduced malignant samples into one DataFrame\n",
    "malign_reduced_data = pd.concat(malign_reduced_samples_by_label.values())\n",
    "\n",
    "# Drop the 'prob' column\n",
    "malign_reduced_data = malign_reduced_data.drop('prob', axis=1)\n",
    "\n",
    "# Concatenate all benign samples into one DataFrame\n",
    "benign_data = benign_samples\n",
    "\n",
    "# Concatenate all benign and reduced malignant samples into one DataFrame\n",
    "all_data = pd.concat([benign_data, malign_reduced_data])\n",
    "\n",
    "print(f\"Total samples in final dataset: {len(all_data)}\")\n",
    "print(f\"Number of benign samples: {len(benign_data)}\")\n",
    "print(f\"Number of malignant samples: {len(malign_reduced_data)}\")\n",
    "print(\"\\nSample distribution:\")\n",
    "print(all_data.iloc[:, get_label_column_index(all_data)].value_counts())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in final dataset: 17313534\n",
      "Number of benign samples: 2602\n",
      "Number of malignant samples: 17310932\n",
      "\n",
      "Sample distribution:\n",
      "Label\n",
      "DoS RST                  2145008\n",
      "Information Gathering    2076726\n",
      "DoS ACK                  1872614\n",
      "DoS PSH                  1819014\n",
      "DoS URG                  1812380\n",
      "DoS CWR                  1745046\n",
      "DoS ECN                  1742300\n",
      "DoS SYN                  1713528\n",
      "DoS FIN                  1451200\n",
      "DoS UDP                   515988\n",
      "DoS HTTP                  164702\n",
      "Mirai DDoS DNS            110392\n",
      "Bruteforce DNS             44358\n",
      "Mirai DDoS SYN             28420\n",
      "Mirai DDoS HTTP            17846\n",
      "Mirai Scan Bruteforce      17462\n",
      "Bruteforce Telnet           9826\n",
      "Bruteforce SSH              7934\n",
      "Mirai DDoS ACK              7558\n",
      "Bruteforce FTP              6970\n",
      "BENIGN                      2602\n",
      "Bruteforce HTTP             1256\n",
      "Mirai DDoS UDP               142\n",
      "Mirai DDoS GREIP              98\n",
      "Mirai DDoS GREETH             86\n",
      "DoS MAC                       60\n",
      "DoS ICMP                      18\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### PRUEBA SIN REDUCIR DATASET MALIGNO. DEBERIASMOS REDUCIRLO PARA QUE SEA MAS BALANCEADO EN EL CODIGO ANTERIOR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Asumimos que data_frames, benign_samples, y get_label_column_index están definidos previamente\n",
    "\n",
    "# Gaussian distribution parameters\n",
    "mu = 0.5\n",
    "sigma = 0.2\n",
    "\n",
    "# Initialize dictionary for malignant samples by label\n",
    "malign_samples_by_label = {}\n",
    "\n",
    "# Process each DataFrame\n",
    "for file, df in data_frames.items():\n",
    "    label_col_index = get_label_column_index(df)\n",
    "    \n",
    "    # Standardize the label column by replacing 'UDP-lag' and 'UDP-Lag' with 'UDPLag'\n",
    "    df.iloc[:, label_col_index] = df.iloc[:, label_col_index].replace(['UDP-lag', 'UDP-Lag'], 'UDPLag')\n",
    "    \n",
    "    # Get only malignant samples and drop of WEBDDOS\n",
    "    malign_samples = df[(df.iloc[:, label_col_index] != 'BENIGN') & (df.iloc[:, label_col_index] != 'WebDDoS')]\n",
    "    \n",
    "    # Group by label and collect samples\n",
    "    for label in malign_samples.iloc[:, label_col_index].unique():\n",
    "        if label not in malign_samples_by_label:\n",
    "            malign_samples_by_label[label] = pd.DataFrame()\n",
    "        \n",
    "        malign_samples_by_label[label] = pd.concat([malign_samples_by_label[label], malign_samples[malign_samples.iloc[:, label_col_index] == label]])\n",
    "\n",
    "# Concatenate all malignant samples into one DataFrame\n",
    "malign_data = pd.concat(malign_samples_by_label.values())\n",
    "\n",
    "# Concatenate all benign and malignant samples into one DataFrame\n",
    "all_data = pd.concat([benign_samples, malign_data])\n",
    "\n",
    "print(f\"Total samples in final dataset: {len(all_data)}\")\n",
    "print(f\"Number of benign samples: {len(benign_samples)}\")\n",
    "print(f\"Number of malignant samples: {len(malign_data)}\")\n",
    "print(\"\\nSample distribution:\")\n",
    "print(all_data.iloc[:, get_label_column_index(all_data)].value_counts())\n",
    "\n",
    "# Now you can use all_data for further processing or analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nimport os\\n\\n# Definir el mapeo de categorías\\ncategory_mapping = {\\n    \\'BENIGN\\': \\'BENIGN\\',\\n    \\'Bruteforce DNS\\': \\'Bruteforce\\',\\n    \\'Bruteforce FTP\\': \\'Bruteforce\\',\\n    \\'Bruteforce HTTP\\': \\'Bruteforce\\',\\n    \\'Bruteforce SSH\\': \\'Bruteforce\\',\\n    \\'Bruteforce Telnet\\': \\'Bruteforce\\',\\n    \\'DoS ACK\\': \\'DoS\\',\\n    \\'DoS CWR\\': \\'DoS\\',\\n    \\'DoS ECN\\': \\'DoS\\',\\n    \\'DoS FIN\\': \\'DoS\\',\\n    \\'DoS HTTP\\': \\'DoS\\',\\n    \\'DoS ICMP\\': \\'DoS\\',\\n    \\'DoS MAC\\': \\'DoS\\',\\n    \\'DoS PSH\\': \\'DoS\\',\\n    \\'DoS RST\\': \\'DoS\\',\\n    \\'DoS SYN\\': \\'DoS\\',\\n    \\'DoS UDP\\': \\'DoS\\',\\n    \\'DoS URG\\': \\'DoS\\',\\n    \\'Information Gathering\\': \\'Information Gathering\\',\\n    \\'Mirai DDoS ACK\\': \\'Mirai\\',\\n    \\'Mirai DDoS DNS\\': \\'Mirai\\',\\n    \\'Mirai DDoS GREETH\\': \\'Mirai\\',\\n    \\'Mirai DDoS GREIP\\': \\'Mirai\\',\\n    \\'Mirai DDoS HTTP\\': \\'Mirai\\',\\n    \\'Mirai DDoS SYN\\': \\'Mirai\\',\\n    \\'Mirai DDoS UDP\\': \\'Mirai\\',\\n    \\'Mirai Scan Bruteforce\\': \\'Mirai\\'\\n}\\n\\n# Función para procesar el dataset en lotes\\ndef process_data_in_batches(input_file, output_file, batch_size=100000):\\n    # Inicializar contadores y almacenamiento temporal\\n    category_counts = {}\\n    temp_files = []\\n    \\n    # Procesar el archivo de entrada en lotes\\n    for chunk in pd.read_csv(input_file, chunksize=batch_size):\\n        # Aplicar el mapeo de categorías\\n        label_column_index = get_label_column_index(chunk)\\n        chunk.iloc[:, label_column_index] = chunk.iloc[:, label_column_index].map(category_mapping)\\n        \\n        # Actualizar conteos de categorías\\n        chunk_counts = chunk.iloc[:, label_column_index].value_counts()\\n        for category, count in chunk_counts.items():\\n            if category not in category_counts:\\n                category_counts[category] = 0\\n            category_counts[category] += count\\n        \\n        # Guardar el chunk procesado en un archivo temporal\\n        temp_file = f\\'temp_chunk_{len(temp_files)}.csv\\'\\n        chunk.to_csv(temp_file, index=False)\\n        temp_files.append(temp_file)\\n    \\n    # Encontrar la categoría con más muestras (excluyendo BENIGN)\\n    max_samples = max(count for category, count in category_counts.items() if category != \\'BENIGN\\')\\n    \\n    # Procesar y balancear los datos\\n    with open(output_file, \\'w\\') as outfile:\\n        first_chunk = True\\n        for temp_file in temp_files:\\n            for chunk in pd.read_csv(temp_file, chunksize=batch_size):\\n                # Separar BENIGN del resto\\n                benign = chunk[chunk.iloc[:, label_column_index] == \\'BENIGN\\']\\n                malicious = chunk[chunk.iloc[:, label_column_index] != \\'BENIGN\\']\\n                \\n                # Sobremuestrear categorías maliciosas\\n                balanced_malicious = pd.DataFrame()\\n                for category in malicious.iloc[:, label_column_index].unique():\\n                    category_data = malicious[malicious.iloc[:, label_column_index] == category]\\n                    samples_needed = max_samples - category_counts[category]\\n                    if samples_needed > 0:\\n                        oversampled = category_data.sample(n=samples_needed, replace=True, random_state=42)\\n                        balanced_malicious = pd.concat([balanced_malicious, category_data, oversampled])\\n                        category_counts[category] = max_samples\\n                    else:\\n                        balanced_malicious = pd.concat([balanced_malicious, category_data])\\n                \\n                # Combinar BENIGN y datos maliciosos balanceados\\n                balanced_chunk = pd.concat([benign, balanced_malicious])\\n                \\n                # Escribir en el archivo de salida\\n                balanced_chunk.to_csv(outfile, mode=\\'a\\', header=first_chunk, index=False)\\n                first_chunk = False\\n            \\n            # Eliminar el archivo temporal después de procesarlo\\n            os.remove(temp_file)\\n    \\n    return category_counts\\n\\n# Procesar y balancear los datos\\ninput_file = \\'data.csv\\'  # Reemplazar con el nombre de tu archivo de entrada\\noutput_file = \\'balanced_reduced_categories_dataset.csv\\'\\nfinal_category_counts = process_data_in_batches(input_file, output_file)\\n\\nprint(\"Distribución final de categorías:\")\\nfor category, count in final_category_counts.items():\\n    print(f\"{category}: {count}\")\\n\\n# Dividir los datos en conjuntos de entrenamiento y prueba\\ndef split_data(input_file, train_file, test_file, test_size=0.3, batch_size=100000):\\n    # Leer el archivo de entrada en lotes y escribir en archivos de entrenamiento y prueba\\n    for chunk in pd.read_csv(input_file, chunksize=batch_size):\\n        train, test = train_test_split(chunk, test_size=test_size, stratify=chunk.iloc[:, get_label_column_index(chunk)])\\n        train.to_csv(train_file, mode=\\'a\\', header=not os.path.exists(train_file), index=False)\\n        test.to_csv(test_file, mode=\\'a\\', header=not os.path.exists(test_file), index=False)\\n\\n# Dividir los datos\\ntrain_file = \\'train_balanced_reduced_categories.csv\\'\\ntest_file = \\'test_balanced_reduced_categories.csv\\'\\nsplit_data(output_file, train_file, test_file)\\n\\nprint(\"\\nLos conjuntos de datos han sido procesados, balanceados y divididos en archivos de entrenamiento y prueba.\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Definir el mapeo de categorías\n",
    "category_mapping = {\n",
    "    'BENIGN': 'BENIGN',\n",
    "    'Bruteforce DNS': 'Bruteforce',\n",
    "    'Bruteforce FTP': 'Bruteforce',\n",
    "    'Bruteforce HTTP': 'Bruteforce',\n",
    "    'Bruteforce SSH': 'Bruteforce',\n",
    "    'Bruteforce Telnet': 'Bruteforce',\n",
    "    'DoS ACK': 'DoS',\n",
    "    'DoS CWR': 'DoS',\n",
    "    'DoS ECN': 'DoS',\n",
    "    'DoS FIN': 'DoS',\n",
    "    'DoS HTTP': 'DoS',\n",
    "    'DoS ICMP': 'DoS',\n",
    "    'DoS MAC': 'DoS',\n",
    "    'DoS PSH': 'DoS',\n",
    "    'DoS RST': 'DoS',\n",
    "    'DoS SYN': 'DoS',\n",
    "    'DoS UDP': 'DoS',\n",
    "    'DoS URG': 'DoS',\n",
    "    'Information Gathering': 'Information Gathering',\n",
    "    'Mirai DDoS ACK': 'Mirai',\n",
    "    'Mirai DDoS DNS': 'Mirai',\n",
    "    'Mirai DDoS GREETH': 'Mirai',\n",
    "    'Mirai DDoS GREIP': 'Mirai',\n",
    "    'Mirai DDoS HTTP': 'Mirai',\n",
    "    'Mirai DDoS SYN': 'Mirai',\n",
    "    'Mirai DDoS UDP': 'Mirai',\n",
    "    'Mirai Scan Bruteforce': 'Mirai'\n",
    "}\n",
    "\n",
    "# Función para procesar el dataset en lotes\n",
    "def process_data_in_batches(input_file, output_file, batch_size=100000):\n",
    "    # Inicializar contadores y almacenamiento temporal\n",
    "    category_counts = {}\n",
    "    temp_files = []\n",
    "    \n",
    "    # Procesar el archivo de entrada en lotes\n",
    "    for chunk in pd.read_csv(input_file, chunksize=batch_size):\n",
    "        # Aplicar el mapeo de categorías\n",
    "        label_column_index = get_label_column_index(chunk)\n",
    "        chunk.iloc[:, label_column_index] = chunk.iloc[:, label_column_index].map(category_mapping)\n",
    "        \n",
    "        # Actualizar conteos de categorías\n",
    "        chunk_counts = chunk.iloc[:, label_column_index].value_counts()\n",
    "        for category, count in chunk_counts.items():\n",
    "            if category not in category_counts:\n",
    "                category_counts[category] = 0\n",
    "            category_counts[category] += count\n",
    "        \n",
    "        # Guardar el chunk procesado en un archivo temporal\n",
    "        temp_file = f'temp_chunk_{len(temp_files)}.csv'\n",
    "        chunk.to_csv(temp_file, index=False)\n",
    "        temp_files.append(temp_file)\n",
    "    \n",
    "    # Encontrar la categoría con más muestras (excluyendo BENIGN)\n",
    "    max_samples = max(count for category, count in category_counts.items() if category != 'BENIGN')\n",
    "    \n",
    "    # Procesar y balancear los datos\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        first_chunk = True\n",
    "        for temp_file in temp_files:\n",
    "            for chunk in pd.read_csv(temp_file, chunksize=batch_size):\n",
    "                # Separar BENIGN del resto\n",
    "                benign = chunk[chunk.iloc[:, label_column_index] == 'BENIGN']\n",
    "                malicious = chunk[chunk.iloc[:, label_column_index] != 'BENIGN']\n",
    "                \n",
    "                # Sobremuestrear categorías maliciosas\n",
    "                balanced_malicious = pd.DataFrame()\n",
    "                for category in malicious.iloc[:, label_column_index].unique():\n",
    "                    category_data = malicious[malicious.iloc[:, label_column_index] == category]\n",
    "                    samples_needed = max_samples - category_counts[category]\n",
    "                    if samples_needed > 0:\n",
    "                        oversampled = category_data.sample(n=samples_needed, replace=True, random_state=42)\n",
    "                        balanced_malicious = pd.concat([balanced_malicious, category_data, oversampled])\n",
    "                        category_counts[category] = max_samples\n",
    "                    else:\n",
    "                        balanced_malicious = pd.concat([balanced_malicious, category_data])\n",
    "                \n",
    "                # Combinar BENIGN y datos maliciosos balanceados\n",
    "                balanced_chunk = pd.concat([benign, balanced_malicious])\n",
    "                \n",
    "                # Escribir en el archivo de salida\n",
    "                balanced_chunk.to_csv(outfile, mode='a', header=first_chunk, index=False)\n",
    "                first_chunk = False\n",
    "            \n",
    "            # Eliminar el archivo temporal después de procesarlo\n",
    "            os.remove(temp_file)\n",
    "    \n",
    "    return category_counts\n",
    "\n",
    "# Procesar y balancear los datos\n",
    "input_file = 'data.csv'  # Reemplazar con el nombre de tu archivo de entrada\n",
    "output_file = 'balanced_reduced_categories_dataset.csv'\n",
    "final_category_counts = process_data_in_batches(input_file, output_file)\n",
    "\n",
    "print(\"Distribución final de categorías:\")\n",
    "for category, count in final_category_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "def split_data(input_file, train_file, test_file, test_size=0.3, batch_size=100000):\n",
    "    # Leer el archivo de entrada en lotes y escribir en archivos de entrenamiento y prueba\n",
    "    for chunk in pd.read_csv(input_file, chunksize=batch_size):\n",
    "        train, test = train_test_split(chunk, test_size=test_size, stratify=chunk.iloc[:, get_label_column_index(chunk)])\n",
    "        train.to_csv(train_file, mode='a', header=not os.path.exists(train_file), index=False)\n",
    "        test.to_csv(test_file, mode='a', header=not os.path.exists(test_file), index=False)\n",
    "\n",
    "# Dividir los datos\n",
    "train_file = 'train_balanced_reduced_categories.csv'\n",
    "test_file = 'test_balanced_reduced_categories.csv'\n",
    "split_data(output_file, train_file, test_file)\n",
    "\n",
    "print(\"\\nLos conjuntos de datos han sido procesados, balanceados y divididos en archivos de entrenamiento y prueba.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'test_1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m train_data\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Guardar los conjuntos de prueba en archivos CSV\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtest_val_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_1.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ecaba\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3891\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3893\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3894\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3895\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3899\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3900\u001b[0m )\n\u001b[1;32m-> 3902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3905\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3907\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ecaba\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1134\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1135\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1151\u001b[0m )\n\u001b[1;32m-> 1152\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\ecaba\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:247\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    257\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    258\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\ecaba\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'test_1.csv'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba (70% entrenamiento, 30% prueba)\n",
    "train_data, test_val_data = train_test_split(all_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Dividir el conjunto de prueba en conjuntos de prueba y validación (50% prueba, 50% validación)\n",
    "#test_data, val_data = train_test_split(test_val_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Guardar los conjuntos de entrenamiento en archivos CSV\n",
    "train_data.to_csv('train_1.csv', index=False)\n",
    "\n",
    "# Guardar los conjuntos de prueba en archivos CSV\n",
    "test_val_data.to_csv('test_1.csv', index=False)\n",
    "\n",
    "# Guardar los conjuntos de validación en archivos CSV\n",
    "#val_data.to_csv('validation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
