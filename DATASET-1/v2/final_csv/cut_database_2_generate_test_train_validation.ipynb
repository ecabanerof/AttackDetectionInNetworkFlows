{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/TFM/DATASET-1/v2/final_csv\\data.csv\n",
      "File: data.csv, Shape: (8656767, 77)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Obtener la lista de rutas de los archivos CSV\n",
    "dspaths = []\n",
    "for dirname, _, filenames in os.walk('/TFM/DATASET-1/v2/final_csv'):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.csv'):\n",
    "            pds = os.path.join(dirname, filename)\n",
    "            dspaths.append(pds)\n",
    "            print(pds)\n",
    "\n",
    "# Cargar los archivos CSV en un diccionario de DataFrames\n",
    "data_frames = {}\n",
    "for file in dspaths:\n",
    "    file_name = os.path.basename(file)\n",
    "    df = pd.read_csv(file)\n",
    "    print(f\"File: {file_name}, Shape: {df.shape}\")\n",
    "    data_frames[file_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ecaba\\AppData\\Local\\Temp\\ipykernel_26736\\2259264679.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns=column_mapping, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: BENIGN, Number of samples: 1301\n",
      "Label: Bruteforce, Number of malignant samples: 35172\n",
      "Label: DoS, Number of malignant samples: 7490929\n",
      "Label: Information Gathering, Number of malignant samples: 1038363\n",
      "Label: Mirai, Number of malignant samples: 91002\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to normalize column names\n",
    "def normalize_column_names(df):\n",
    "    column_mapping = {\n",
    "        'Fwd Bytes/Bulk Avg': 'Fwd Avg Bytes/Bulk',\n",
    "        'Fwd Packet/Bulk Avg': 'Fwd Avg Packets/Bulk',\n",
    "        'Fwd Packets/Bulk Avg': 'Fwd Avg Packets/Bulk', # Duplicate column name not really present\n",
    "        'Fwd Bulk Rate Avg': 'Fwd Avg Bulk Rate',\n",
    "        'Bwd Bytes/Bulk Avg': 'Bwd Avg Bytes/Bulk',\n",
    "        'Bwd Packet/Bulk Avg': 'Bwd Avg Packets/Bulk',\n",
    "        'Bwd Packets/Bulk Avg': 'Bwd Avg Packets/Bulk', # Duplicate column name not really present\n",
    "        'Bwd Bulk Rate Avg': 'Bwd Avg Bulk Rate',\n",
    "        'FWD Init Win Bytes': 'Init_Win_bytes_forward', # Column changes 2\n",
    "        'Bwd Init Win Bytes': 'Init_Win_bytes_backward',\n",
    "        'Fwd Act Data Pkts': 'act_data_pkt_fwd',\n",
    "        'Fwd Seg Size Min': 'min_seg_size_forward' ,\n",
    "        'Total Fwd Packet': 'Total Fwd Packets',\n",
    "        'Total Bwd packets': 'Total Backward Packets',\n",
    "        'Total Length of Fwd Packet': 'Total Length of Fwd Packets',\n",
    "        'Total Length of Bwd Packet': 'Total Length of Bwd Packets',\n",
    "        'Fwd Segment Size Avg': 'Avg Fwd Segment Size',\n",
    "        'Bwd Segment Size Avg': 'Avg Bwd Segment Size',\n",
    "        'Packet Length Min': 'Min Packet Length',\n",
    "        'Packet Length Max': 'Max Packet Length'\n",
    "    }\n",
    "    df.rename(columns=column_mapping, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Function to get the index of the label column\n",
    "def get_label_column_index(df):\n",
    "    if 'Label' in df.columns:\n",
    "        return df.columns.get_loc('Label')\n",
    "    return df.shape[1] - 1\n",
    "\n",
    "# Crear un DataFrame con todas las muestras benignas\n",
    "benign_samples = pd.concat([normalize_column_names(df[df.iloc[:, get_label_column_index(df)] == 'BENIGN']) for df in data_frames.values()])\n",
    "print(f\"Label: BENIGN, Number of samples: {len(benign_samples)}\")\n",
    "\n",
    "# Gaussian distribution parameters\n",
    "mu = 0.5\n",
    "sigma = 0.2\n",
    "\n",
    "# Initialize dictionary for malignant samples by label\n",
    "malign_samples_by_label = {}\n",
    "\n",
    "# Define the category mapping\n",
    "category_mapping = {\n",
    "    'BENIGN': 'BENIGN',\n",
    "    'Bruteforce DNS': 'Bruteforce',\n",
    "    'Bruteforce FTP': 'Bruteforce',\n",
    "    'Bruteforce HTTP': 'Bruteforce',\n",
    "    'Bruteforce SSH': 'Bruteforce',\n",
    "    'Bruteforce Telnet': 'Bruteforce',\n",
    "    'DoS ACK': 'DoS',\n",
    "    'DoS CWR': 'DoS',\n",
    "    'DoS ECN': 'DoS',\n",
    "    'DoS FIN': 'DoS',\n",
    "    'DoS HTTP': 'DoS',\n",
    "    'DoS ICMP': 'DoS',\n",
    "    'DoS MAC': 'DoS',\n",
    "    'DoS PSH': 'DoS',\n",
    "    'DoS RST': 'DoS',\n",
    "    'DoS SYN': 'DoS',\n",
    "    'DoS UDP': 'DoS',\n",
    "    'DoS URG': 'DoS',\n",
    "    'Information Gathering': 'Information Gathering',\n",
    "    'Mirai DDoS ACK': 'Mirai',\n",
    "    'Mirai DDoS DNS': 'Mirai',\n",
    "    'Mirai DDoS GREETH': 'Mirai',\n",
    "    'Mirai DDoS GREIP': 'Mirai',\n",
    "    'Mirai DDoS HTTP': 'Mirai',\n",
    "    'Mirai DDoS SYN': 'Mirai',\n",
    "    'Mirai DDoS UDP': 'Mirai',\n",
    "    'Mirai Scan Bruteforce': 'Mirai'\n",
    "}\n",
    "\n",
    "# Process each DataFrame\n",
    "for file, df in data_frames.items():\n",
    "    # Normalize column names\n",
    "    df = normalize_column_names(df)\n",
    "    \n",
    "    label_col_index = get_label_column_index(df)\n",
    "    \n",
    "    # Standardize the label column by replacing 'UDP-lag' and 'UDP-Lag' with 'UDPLag'\n",
    "    df.iloc[:, label_col_index] = df.iloc[:, label_col_index].replace(['UDP-lag', 'UDP-Lag'], 'UDPLag')\n",
    "    \n",
    "    # Apply category mapping\n",
    "    df.iloc[:, label_col_index] = df.iloc[:, label_col_index].map(category_mapping)\n",
    "    \n",
    "    # Get only malignant samples and drop of WEBDDOS\n",
    "    malign_samples = df[(df.iloc[:, label_col_index] != 'BENIGN') & (df.iloc[:, label_col_index] != 'WebDDoS')]\n",
    "    \n",
    "    # Group by label and collect samples\n",
    "    for label in malign_samples.iloc[:, label_col_index].unique():\n",
    "        if label not in malign_samples_by_label:\n",
    "            malign_samples_by_label[label] = pd.DataFrame()\n",
    "        \n",
    "        malign_samples_by_label[label] = pd.concat([malign_samples_by_label[label], malign_samples[malign_samples.iloc[:, label_col_index] == label]])\n",
    "\n",
    "# Display the number of malignant samples per label\n",
    "for label, samples in malign_samples_by_label.items():\n",
    "    malign_count = len(samples)\n",
    "    print(f\"Label: {label}, Number of malignant samples: {malign_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducción de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom scipy.stats import norm\\n\\n# Gaussian distribution parameters\\nmu = 0.5\\nsigma = 0.2\\n\\n# Initialize dictionary for reduced malignant samples by label\\nmalign_reduced_samples_by_label = {}\\n\\n# Process each DataFrame\\nfor label, samples in malign_samples_by_label.items():\\n    # Calculate the probability density function of each sample\\n    samples[\\'prob\\'] = norm.pdf(np.linspace(0, 1, len(samples)), mu, sigma)\\n    \\n    # Reduce samples to 113828 using the probability weights\\n    if len(samples) > 113828:\\n        malign_reduced_samples_by_label[label] = samples.sample(n=113828, weights=\\'prob\\', random_state=42)\\n    #else:\\n        malign_reduced_samples_by_label[label] = samples\\n    \\n    print(f\"Label: {label}, Original samples: {len(samples)}, Reduced samples: {len(malign_reduced_samples_by_label[label])}\")\\n\\n# Concatenate all reduced malignant samples into one DataFrame\\nmalign_reduced_data = pd.concat(malign_reduced_samples_by_label.values())\\n\\n# Drop the \\'prob\\' column\\nmalign_reduced_data = malign_reduced_data.drop(\\'prob\\', axis=1)\\n\\n# Concatenate all benign samples into one DataFrame\\nbenign_data = benign_samples\\n\\n# Concatenate all benign and reduced malignant samples into one DataFrame\\nall_data = pd.concat([benign_data, malign_reduced_data])\\n\\nprint(f\"Total samples in final dataset: {len(all_data)}\")\\nprint(f\"Number of benign samples: {len(benign_data)}\")\\nprint(f\"Number of malignant samples: {len(malign_reduced_data)}\")\\nprint(\"\\nSample distribution:\")\\nprint(all_data.iloc[:, get_label_column_index(all_data)].value_counts())\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Gaussian distribution parameters\n",
    "mu = 0.5\n",
    "sigma = 0.2\n",
    "\n",
    "# Initialize dictionary for reduced malignant samples by label\n",
    "malign_reduced_samples_by_label = {}\n",
    "\n",
    "# Process each DataFrame\n",
    "for label, samples in malign_samples_by_label.items():\n",
    "    # Calculate the probability density function of each sample\n",
    "    samples['prob'] = norm.pdf(np.linspace(0, 1, len(samples)), mu, sigma)\n",
    "    \n",
    "    # Reduce samples to 113828 using the probability weights\n",
    "    if len(samples) > 113828:\n",
    "        malign_reduced_samples_by_label[label] = samples.sample(n=113828, weights='prob', random_state=42)\n",
    "    #else:\n",
    "        malign_reduced_samples_by_label[label] = samples\n",
    "    \n",
    "    print(f\"Label: {label}, Original samples: {len(samples)}, Reduced samples: {len(malign_reduced_samples_by_label[label])}\")\n",
    "\n",
    "# Concatenate all reduced malignant samples into one DataFrame\n",
    "malign_reduced_data = pd.concat(malign_reduced_samples_by_label.values())\n",
    "\n",
    "# Drop the 'prob' column\n",
    "malign_reduced_data = malign_reduced_data.drop('prob', axis=1)\n",
    "\n",
    "# Concatenate all benign samples into one DataFrame\n",
    "benign_data = benign_samples\n",
    "\n",
    "# Concatenate all benign and reduced malignant samples into one DataFrame\n",
    "all_data = pd.concat([benign_data, malign_reduced_data])\n",
    "\n",
    "print(f\"Total samples in final dataset: {len(all_data)}\")\n",
    "print(f\"Number of benign samples: {len(benign_data)}\")\n",
    "print(f\"Number of malignant samples: {len(malign_reduced_data)}\")\n",
    "print(\"\\nSample distribution:\")\n",
    "print(all_data.iloc[:, get_label_column_index(all_data)].value_counts())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Bruteforce, Number of malignant samples: 35172\n",
      "Label: DoS, Number of malignant samples: 7490929\n",
      "Label: Information Gathering, Number of malignant samples: 1038363\n",
      "Label: Mirai, Number of malignant samples: 91002\n",
      "Total samples in final dataset: 601301\n",
      "Number of benign samples: 1301\n",
      "Number of malignant samples: 600000\n",
      "\n",
      "Sample distribution:\n",
      "Label\n",
      "DoS                      200000\n",
      "Information Gathering    200000\n",
      "Bruteforce               100000\n",
      "Mirai                    100000\n",
      "BENIGN                     1301\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### PRUEBA REDUCIENDO DATASET MALIGNO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Asumimos que data_frames, benign_samples, y get_label_column_index están definidos previamente\n",
    "\n",
    "# Gaussian distribution parameters\n",
    "mu = 0.5\n",
    "sigma = 0.2\n",
    "\n",
    "# Initialize dictionary for malignant samples by label\n",
    "malign_samples_by_label = {}\n",
    "\n",
    "# Process each DataFrame\n",
    "for file, df in data_frames.items():\n",
    "    label_col_index = get_label_column_index(df)\n",
    "    \n",
    "    # Standardize the label column by replacing 'UDP-lag' and 'UDP-Lag' with 'UDPLag'\n",
    "    df.iloc[:, label_col_index] = df.iloc[:, label_col_index].replace(['UDP-lag', 'UDP-Lag'], 'UDPLag')\n",
    "    \n",
    "    # Get only malignant samples and drop of WEBDDOS\n",
    "    malign_samples = df[(df.iloc[:, label_col_index] != 'BENIGN') & (df.iloc[:, label_col_index] != 'WebDDoS')]\n",
    "    \n",
    "    # Group by label and collect samples\n",
    "    for label in malign_samples.iloc[:, label_col_index].unique():\n",
    "        if label not in malign_samples_by_label:\n",
    "            malign_samples_by_label[label] = pd.DataFrame()\n",
    "        \n",
    "        malign_samples_by_label[label] = pd.concat([malign_samples_by_label[label], malign_samples[malign_samples.iloc[:, label_col_index] == label]])\n",
    "\n",
    "\n",
    "\n",
    "# Display the number of malignant samples per label\n",
    "for label, samples in malign_samples_by_label.items():\n",
    "    malign_count = len(samples)\n",
    "    print(f\"Label: {label}, Number of malignant samples: {malign_count}\")\n",
    "\n",
    "# Reduce or increase samples as specified\n",
    "target_samples = {\n",
    "    'DoS': 200000,\n",
    "    'Information Gathering': 200000,\n",
    "    'Mirai': 100000,\n",
    "    'Bruteforce': 100000\n",
    "}\n",
    "\n",
    "for label, target in target_samples.items():\n",
    "    if label in malign_samples_by_label:\n",
    "        current_samples = len(malign_samples_by_label[label])\n",
    "        if current_samples > target:\n",
    "            # Reduce samples\n",
    "            malign_samples_by_label[label] = malign_samples_by_label[label].sample(n=target, random_state=42)\n",
    "        elif current_samples < target:\n",
    "            # Increase samples\n",
    "            additional_samples = target - current_samples\n",
    "            malign_samples_by_label[label] = pd.concat([malign_samples_by_label[label], malign_samples_by_label[label].sample(n=additional_samples, replace=True, random_state=42)])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all malignant samples into one DataFrame\n",
    "malign_data = pd.concat(malign_samples_by_label.values())\n",
    "\n",
    "# Concatenate all benign and malignant samples into one DataFrame\n",
    "all_data = pd.concat([benign_samples, malign_data])\n",
    "\n",
    "print(f\"Total samples in final dataset: {len(all_data)}\")\n",
    "print(f\"Number of benign samples: {len(benign_samples)}\")\n",
    "print(f\"Number of malignant samples: {len(malign_data)}\")\n",
    "print(\"\\nSample distribution:\")\n",
    "print(all_data.iloc[:, get_label_column_index(all_data)].value_counts())\n",
    "\n",
    "# Now you can use all_data for further processing or analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nimport os\\n\\n# Definir el mapeo de categorías\\ncategory_mapping = {\\n    \\'BENIGN\\': \\'BENIGN\\',\\n    \\'Bruteforce DNS\\': \\'Bruteforce\\',\\n    \\'Bruteforce FTP\\': \\'Bruteforce\\',\\n    \\'Bruteforce HTTP\\': \\'Bruteforce\\',\\n    \\'Bruteforce SSH\\': \\'Bruteforce\\',\\n    \\'Bruteforce Telnet\\': \\'Bruteforce\\',\\n    \\'DoS ACK\\': \\'DoS\\',\\n    \\'DoS CWR\\': \\'DoS\\',\\n    \\'DoS ECN\\': \\'DoS\\',\\n    \\'DoS FIN\\': \\'DoS\\',\\n    \\'DoS HTTP\\': \\'DoS\\',\\n    \\'DoS ICMP\\': \\'DoS\\',\\n    \\'DoS MAC\\': \\'DoS\\',\\n    \\'DoS PSH\\': \\'DoS\\',\\n    \\'DoS RST\\': \\'DoS\\',\\n    \\'DoS SYN\\': \\'DoS\\',\\n    \\'DoS UDP\\': \\'DoS\\',\\n    \\'DoS URG\\': \\'DoS\\',\\n    \\'Information Gathering\\': \\'Information Gathering\\',\\n    \\'Mirai DDoS ACK\\': \\'Mirai\\',\\n    \\'Mirai DDoS DNS\\': \\'Mirai\\',\\n    \\'Mirai DDoS GREETH\\': \\'Mirai\\',\\n    \\'Mirai DDoS GREIP\\': \\'Mirai\\',\\n    \\'Mirai DDoS HTTP\\': \\'Mirai\\',\\n    \\'Mirai DDoS SYN\\': \\'Mirai\\',\\n    \\'Mirai DDoS UDP\\': \\'Mirai\\',\\n    \\'Mirai Scan Bruteforce\\': \\'Mirai\\'\\n}\\n\\n# Función para procesar el dataset en lotes\\ndef process_data_in_batches(input_file, output_file, batch_size=100000):\\n    # Inicializar contadores y almacenamiento temporal\\n    category_counts = {}\\n    temp_files = []\\n    \\n    # Procesar el archivo de entrada en lotes\\n    for chunk in pd.read_csv(input_file, chunksize=batch_size):\\n        # Aplicar el mapeo de categorías\\n        label_column_index = get_label_column_index(chunk)\\n        chunk.iloc[:, label_column_index] = chunk.iloc[:, label_column_index].map(category_mapping)\\n        \\n        # Actualizar conteos de categorías\\n        chunk_counts = chunk.iloc[:, label_column_index].value_counts()\\n        for category, count in chunk_counts.items():\\n            if category not in category_counts:\\n                category_counts[category] = 0\\n            category_counts[category] += count\\n        \\n        # Guardar el chunk procesado en un archivo temporal\\n        temp_file = f\\'temp_chunk_{len(temp_files)}.csv\\'\\n        chunk.to_csv(temp_file, index=False)\\n        temp_files.append(temp_file)\\n    \\n    # Encontrar la categoría con más muestras (excluyendo BENIGN)\\n    max_samples = max(count for category, count in category_counts.items() if category != \\'BENIGN\\')\\n    \\n    # Procesar y balancear los datos\\n    with open(output_file, \\'w\\') as outfile:\\n        first_chunk = True\\n        for temp_file in temp_files:\\n            for chunk in pd.read_csv(temp_file, chunksize=batch_size):\\n                # Separar BENIGN del resto\\n                benign = chunk[chunk.iloc[:, label_column_index] == \\'BENIGN\\']\\n                malicious = chunk[chunk.iloc[:, label_column_index] != \\'BENIGN\\']\\n                \\n                # Sobremuestrear categorías maliciosas\\n                balanced_malicious = pd.DataFrame()\\n                for category in malicious.iloc[:, label_column_index].unique():\\n                    category_data = malicious[malicious.iloc[:, label_column_index] == category]\\n                    samples_needed = max_samples - category_counts[category]\\n                    if samples_needed > 0:\\n                        oversampled = category_data.sample(n=samples_needed, replace=True, random_state=42)\\n                        balanced_malicious = pd.concat([balanced_malicious, category_data, oversampled])\\n                        category_counts[category] = max_samples\\n                    else:\\n                        balanced_malicious = pd.concat([balanced_malicious, category_data])\\n                \\n                # Combinar BENIGN y datos maliciosos balanceados\\n                balanced_chunk = pd.concat([benign, balanced_malicious])\\n                \\n                # Escribir en el archivo de salida\\n                balanced_chunk.to_csv(outfile, mode=\\'a\\', header=first_chunk, index=False)\\n                first_chunk = False\\n            \\n            # Eliminar el archivo temporal después de procesarlo\\n            os.remove(temp_file)\\n    \\n    return category_counts\\n\\n# Procesar y balancear los datos\\ninput_file = \\'data.csv\\'  # Reemplazar con el nombre de tu archivo de entrada\\noutput_file = \\'balanced_reduced_categories_dataset.csv\\'\\nfinal_category_counts = process_data_in_batches(input_file, output_file)\\n\\nprint(\"Distribución final de categorías:\")\\nfor category, count in final_category_counts.items():\\n    print(f\"{category}: {count}\")\\n\\n# Dividir los datos en conjuntos de entrenamiento y prueba\\ndef split_data(input_file, train_file, test_file, test_size=0.3, batch_size=100000):\\n    # Leer el archivo de entrada en lotes y escribir en archivos de entrenamiento y prueba\\n    for chunk in pd.read_csv(input_file, chunksize=batch_size):\\n        train, test = train_test_split(chunk, test_size=test_size, stratify=chunk.iloc[:, get_label_column_index(chunk)])\\n        train.to_csv(train_file, mode=\\'a\\', header=not os.path.exists(train_file), index=False)\\n        test.to_csv(test_file, mode=\\'a\\', header=not os.path.exists(test_file), index=False)\\n\\n# Dividir los datos\\ntrain_file = \\'train_balanced_reduced_categories.csv\\'\\ntest_file = \\'test_balanced_reduced_categories.csv\\'\\nsplit_data(output_file, train_file, test_file)\\n\\nprint(\"\\nLos conjuntos de datos han sido procesados, balanceados y divididos en archivos de entrenamiento y prueba.\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Definir el mapeo de categorías\n",
    "category_mapping = {\n",
    "    'BENIGN': 'BENIGN',\n",
    "    'Bruteforce DNS': 'Bruteforce',\n",
    "    'Bruteforce FTP': 'Bruteforce',\n",
    "    'Bruteforce HTTP': 'Bruteforce',\n",
    "    'Bruteforce SSH': 'Bruteforce',\n",
    "    'Bruteforce Telnet': 'Bruteforce',\n",
    "    'DoS ACK': 'DoS',\n",
    "    'DoS CWR': 'DoS',\n",
    "    'DoS ECN': 'DoS',\n",
    "    'DoS FIN': 'DoS',\n",
    "    'DoS HTTP': 'DoS',\n",
    "    'DoS ICMP': 'DoS',\n",
    "    'DoS MAC': 'DoS',\n",
    "    'DoS PSH': 'DoS',\n",
    "    'DoS RST': 'DoS',\n",
    "    'DoS SYN': 'DoS',\n",
    "    'DoS UDP': 'DoS',\n",
    "    'DoS URG': 'DoS',\n",
    "    'Information Gathering': 'Information Gathering',\n",
    "    'Mirai DDoS ACK': 'Mirai',\n",
    "    'Mirai DDoS DNS': 'Mirai',\n",
    "    'Mirai DDoS GREETH': 'Mirai',\n",
    "    'Mirai DDoS GREIP': 'Mirai',\n",
    "    'Mirai DDoS HTTP': 'Mirai',\n",
    "    'Mirai DDoS SYN': 'Mirai',\n",
    "    'Mirai DDoS UDP': 'Mirai',\n",
    "    'Mirai Scan Bruteforce': 'Mirai'\n",
    "}\n",
    "\n",
    "# Función para procesar el dataset en lotes\n",
    "def process_data_in_batches(input_file, output_file, batch_size=100000):\n",
    "    # Inicializar contadores y almacenamiento temporal\n",
    "    category_counts = {}\n",
    "    temp_files = []\n",
    "    \n",
    "    # Procesar el archivo de entrada en lotes\n",
    "    for chunk in pd.read_csv(input_file, chunksize=batch_size):\n",
    "        # Aplicar el mapeo de categorías\n",
    "        label_column_index = get_label_column_index(chunk)\n",
    "        chunk.iloc[:, label_column_index] = chunk.iloc[:, label_column_index].map(category_mapping)\n",
    "        \n",
    "        # Actualizar conteos de categorías\n",
    "        chunk_counts = chunk.iloc[:, label_column_index].value_counts()\n",
    "        for category, count in chunk_counts.items():\n",
    "            if category not in category_counts:\n",
    "                category_counts[category] = 0\n",
    "            category_counts[category] += count\n",
    "        \n",
    "        # Guardar el chunk procesado en un archivo temporal\n",
    "        temp_file = f'temp_chunk_{len(temp_files)}.csv'\n",
    "        chunk.to_csv(temp_file, index=False)\n",
    "        temp_files.append(temp_file)\n",
    "    \n",
    "    # Encontrar la categoría con más muestras (excluyendo BENIGN)\n",
    "    max_samples = max(count for category, count in category_counts.items() if category != 'BENIGN')\n",
    "    \n",
    "    # Procesar y balancear los datos\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        first_chunk = True\n",
    "        for temp_file in temp_files:\n",
    "            for chunk in pd.read_csv(temp_file, chunksize=batch_size):\n",
    "                # Separar BENIGN del resto\n",
    "                benign = chunk[chunk.iloc[:, label_column_index] == 'BENIGN']\n",
    "                malicious = chunk[chunk.iloc[:, label_column_index] != 'BENIGN']\n",
    "                \n",
    "                # Sobremuestrear categorías maliciosas\n",
    "                balanced_malicious = pd.DataFrame()\n",
    "                for category in malicious.iloc[:, label_column_index].unique():\n",
    "                    category_data = malicious[malicious.iloc[:, label_column_index] == category]\n",
    "                    samples_needed = max_samples - category_counts[category]\n",
    "                    if samples_needed > 0:\n",
    "                        oversampled = category_data.sample(n=samples_needed, replace=True, random_state=42)\n",
    "                        balanced_malicious = pd.concat([balanced_malicious, category_data, oversampled])\n",
    "                        category_counts[category] = max_samples\n",
    "                    else:\n",
    "                        balanced_malicious = pd.concat([balanced_malicious, category_data])\n",
    "                \n",
    "                # Combinar BENIGN y datos maliciosos balanceados\n",
    "                balanced_chunk = pd.concat([benign, balanced_malicious])\n",
    "                \n",
    "                # Escribir en el archivo de salida\n",
    "                balanced_chunk.to_csv(outfile, mode='a', header=first_chunk, index=False)\n",
    "                first_chunk = False\n",
    "            \n",
    "            # Eliminar el archivo temporal después de procesarlo\n",
    "            os.remove(temp_file)\n",
    "    \n",
    "    return category_counts\n",
    "\n",
    "# Procesar y balancear los datos\n",
    "input_file = 'data.csv'  # Reemplazar con el nombre de tu archivo de entrada\n",
    "output_file = 'balanced_reduced_categories_dataset.csv'\n",
    "final_category_counts = process_data_in_batches(input_file, output_file)\n",
    "\n",
    "print(\"Distribución final de categorías:\")\n",
    "for category, count in final_category_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "def split_data(input_file, train_file, test_file, test_size=0.3, batch_size=100000):\n",
    "    # Leer el archivo de entrada en lotes y escribir en archivos de entrenamiento y prueba\n",
    "    for chunk in pd.read_csv(input_file, chunksize=batch_size):\n",
    "        train, test = train_test_split(chunk, test_size=test_size, stratify=chunk.iloc[:, get_label_column_index(chunk)])\n",
    "        train.to_csv(train_file, mode='a', header=not os.path.exists(train_file), index=False)\n",
    "        test.to_csv(test_file, mode='a', header=not os.path.exists(test_file), index=False)\n",
    "\n",
    "# Dividir los datos\n",
    "train_file = 'train_balanced_reduced_categories.csv'\n",
    "test_file = 'test_balanced_reduced_categories.csv'\n",
    "split_data(output_file, train_file, test_file)\n",
    "\n",
    "print(\"\\nLos conjuntos de datos han sido procesados, balanceados y divididos en archivos de entrenamiento y prueba.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba (70% entrenamiento, 30% prueba)\n",
    "train_data, test_val_data = train_test_split(all_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Dividir el conjunto de prueba en conjuntos de prueba y validación (50% prueba, 50% validación)\n",
    "test_data, val_data = train_test_split(test_val_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Guardar los conjuntos de entrenamiento en archivos CSV\n",
    "train_data.to_csv('train_1.csv', index=False)\n",
    "\n",
    "# Guardar los conjuntos de prueba en archivos CSV\n",
    "test_val_data.to_csv('test_1.csv', index=False)\n",
    "\n",
    "# Guardar los conjuntos de validación en archivos CSV\n",
    "val_data.to_csv('validation_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
